<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Related topics | Statistical Inference via Data Science</title>
  <meta name="description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Related topics | Statistical Inference via Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://moderndive.com/" />
  <meta property="og:image" content="https://moderndive.com/images/logos/book_cover.png" />
  <meta property="og:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="github-repo" content="moderndive/ModernDive_book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Related topics | Statistical Inference via Data Science" />
  <meta name="twitter:site" content="@ModernDive" />
  <meta name="twitter:description" content="An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools." />
  <meta name="twitter:image" content="https://moderndive.com/images/logos/book_cover.png" />

<meta name="author" content="Chester Ismay and Albert Y. Kim   Foreword by Kelly S. McConville" />


<meta name="date" content="2021-02-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="images/logos/favicons/apple-touch-icon.png" />
  <link rel="shortcut icon" href="images/logos/favicons/favicon.ico" type="image/x-icon" />
<link rel="prev" href="1-2-model3.html"/>
<link rel="next" href="1-4-mult-reg-conclusion.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to ModernDive</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="1" data-path="1-multiple-regression.html"><a href="1-multiple-regression.html"><i class="fa fa-check"></i><b>1</b> Multiple Regression</a><ul>
<li class="chapter" data-level="" data-path="1-multiple-regression.html"><a href="1-multiple-regression.html#mult-reg-packages"><i class="fa fa-check"></i>Needed packages</a></li>
<li class="chapter" data-level="1.1" data-path="1-1-model4.html"><a href="1-1-model4.html"><i class="fa fa-check"></i><b>1.1</b> One numerical and one categorical explanatory variable</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-model4.html"><a href="1-1-model4.html#model4EDA"><i class="fa fa-check"></i><b>1.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-model4.html"><a href="1-1-model4.html#model4interactiontable"><i class="fa fa-check"></i><b>1.1.2</b> Interaction model</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-model4.html"><a href="1-1-model4.html#model4table"><i class="fa fa-check"></i><b>1.1.3</b> Parallel slopes model</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-model4.html"><a href="1-1-model4.html#model4points"><i class="fa fa-check"></i><b>1.1.4</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-model3.html"><a href="1-2-model3.html"><i class="fa fa-check"></i><b>1.2</b> Two numerical explanatory variables</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-model3.html"><a href="1-2-model3.html#model3EDA"><i class="fa fa-check"></i><b>1.2.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-model3.html"><a href="1-2-model3.html#model3table"><i class="fa fa-check"></i><b>1.2.2</b> Regression plane</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-model3.html"><a href="1-2-model3.html#model3points"><i class="fa fa-check"></i><b>1.2.3</b> Observed/fitted values and residuals</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-mult-reg-related-topics.html"><a href="1-3-mult-reg-related-topics.html"><i class="fa fa-check"></i><b>1.3</b> Related topics</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-mult-reg-related-topics.html"><a href="1-3-mult-reg-related-topics.html#model-selection"><i class="fa fa-check"></i><b>1.3.1</b> Model selection using visualizations</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-mult-reg-related-topics.html"><a href="1-3-mult-reg-related-topics.html#rsquared"><i class="fa fa-check"></i><b>1.3.2</b> Model selection using R-squared</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-3-mult-reg-related-topics.html"><a href="1-3-mult-reg-related-topics.html#correlationcoefficient2"><i class="fa fa-check"></i><b>1.3.3</b> Correlation coefficient</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-3-mult-reg-related-topics.html"><a href="1-3-mult-reg-related-topics.html#simpsonsparadox"><i class="fa fa-check"></i><b>1.3.4</b> Simpson’s Paradox</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-mult-reg-conclusion.html"><a href="1-4-mult-reg-conclusion.html"><i class="fa fa-check"></i><b>1.4</b> Conclusion</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-4-mult-reg-conclusion.html"><a href="1-4-mult-reg-conclusion.html#additional-resources"><i class="fa fa-check"></i><b>1.4.1</b> Additional resources</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-4-mult-reg-conclusion.html"><a href="1-4-mult-reg-conclusion.html#whats-to-come"><i class="fa fa-check"></i><b>1.4.2</b> What’s to come?</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Inference via Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<html>
<img src='https://moderndive.com/wide_format.png' alt="ModernDive">
</html>
<div id="mult-reg-related-topics" class="section level2">
<h2><span class="header-section-number">1.3</span> Related topics</h2>
<div id="model-selection" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Model selection using visualizations</h3>
<p>When should we use an interaction model versus a parallel slopes model? Recall in Sections <a href="1-1-model4.html#model4interactiontable">1.1.2</a> and <a href="1-1-model4.html#model4table">1.1.3</a> we fit both interaction and parallel slopes models for the outcome variable <span class="math inline">\(y\)</span> (teaching score) using a numerical explanatory variable <span class="math inline">\(x_1\)</span> (age) and a categorical explanatory variable <span class="math inline">\(x_2\)</span> (gender recorded as a binary variable). We compared these models in Figure <a href="1-1-model4.html#fig:numxcatx-comparison">1.3</a>, which we display again now.</p>
<div class="figure" style="text-align: center"><span id="fig:recall-parallel-vs-interaction"></span>
<img src="ModernDive_files/figure-html/recall-parallel-vs-interaction-1.png" alt="Previously seen comparison of interaction and parallel slopes models." width="\textwidth" />
<p class="caption">
FIGURE 1.7: Previously seen comparison of interaction and parallel slopes models.
</p>
</div>
<p>A lot of you might have asked yourselves: “Why would I force the lines to have parallel slopes (as seen in the right-hand plot) when they clearly have different slopes (as seen in the left-hand plot)?”.</p>
<p>The answer lies in a philosophical principle known as “Occam’s Razor.” It states that, “all other things being equal, simpler solutions are more likely to be correct than complex ones.” When viewed in a modeling framework, Occam’s Razor  can be restated as, “all other things being equal, simpler models are to be preferred over complex ones.” In other words, we should only favor the more complex model if the additional complexity is <em>warranted</em>.</p>
<p>Let’s revisit the equations for the regression line for both the interaction and parallel slopes model:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Interaction} &amp;: \widehat{y} = \widehat{\text{score}} = b_0 + b_{\text{age}} \cdot \text{age} + b_{\text{male}} \cdot \mathbb{1}_{\text{is male}}(x) + \\
&amp; \qquad b_{\text{age,male}} \cdot \text{age} \cdot \mathbb{1}_{\text{is male}}\\
\text{Parallel slopes} &amp;: \widehat{y} = \widehat{\text{score}} = b_0 + b_{\text{age}} \cdot \text{age} + b_{\text{male}} \cdot \mathbb{1}_{\text{is male}}(x)
\end{aligned}
\]</span></p>
<p>The interaction model is “more complex” in that there is an additional <span class="math inline">\(b_{\text{age,male}} \cdot \text{age} \cdot \mathbb{1}_{\text{is male}}\)</span> interaction term in the equation not present for the parallel slopes model. Or viewed alternatively, the regression table for the interaction model in Table <a href="1-1-model4.html#tab:regtable-interaction">1.3</a> has <em>four</em> rows, whereas the regression table for the parallel slopes model in Table <a href="1-1-model4.html#tab:regtable-parallel-slopes">1.5</a> has <em>three</em> rows. The question becomes: “Is this additional complexity warranted?”. In this case, it can be argued that this additional complexity is warranted, as evidenced by the clear x-shaped pattern of the two regression lines in the left-hand plot of Figure <a href="1-3-mult-reg-related-topics.html#fig:recall-parallel-vs-interaction">1.7</a>.</p>
<p>However, let’s consider an example where the additional complexity might <em>not</em> be warranted. Let’s consider the <code>MA_schools</code> data included in the <code>moderndive</code> package which contains 2017 data on Massachusetts public high schools provided by the Massachusetts Department of Education. For more details, read the help file for this data by running <code>?MA_schools</code> in the console.</p>
<p>Let’s model the numerical outcome variable <span class="math inline">\(y\)</span>, average SAT math score for a given high school, as a function of two explanatory variables:</p>
<ol style="list-style-type: decimal">
<li>A numerical explanatory variable <span class="math inline">\(x_1\)</span>, the percentage of that high school’s student body that are economically disadvantaged and</li>
<li>A categorical explanatory variable <span class="math inline">\(x_2\)</span>, the school size as measured by enrollment: small (13-341 students), medium (342-541 students), and large (542-4264 students).</li>
</ol>
<p>Let’s create visualizations of both the interaction and parallel slopes model once again and display the output in Figure <a href="1-3-mult-reg-related-topics.html#fig:numxcatx-comparison-2">1.8</a>. Recall from Subsection <a href="1-1-model4.html#model4table">1.1.3</a> that the <code>geom_parallel_slopes()</code> function is a special purpose function included in the <code>moderndive</code> package, since the <code>geom_smooth()</code> method in the <code>ggplot2</code> package does not have a convenient way to plot parallel slopes models.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># Interaction model</span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2"><span class="kw">ggplot</span>(MA_schools, </a>
<a class="sourceLine" id="cb26-3" data-line-number="3">       <span class="kw">aes</span>(<span class="dt">x =</span> perc_disadvan, <span class="dt">y =</span> average_sat_math, <span class="dt">color =</span> size)) <span class="op">+</span></a>
<a class="sourceLine" id="cb26-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.25</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb26-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb26-6" data-line-number="6"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Percent economically disadvantaged&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Math SAT Score&quot;</span>, </a>
<a class="sourceLine" id="cb26-7" data-line-number="7">       <span class="dt">color =</span> <span class="st">&quot;School size&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Interaction model&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="co"># Parallel slopes model</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2"><span class="kw">ggplot</span>(MA_schools, </a>
<a class="sourceLine" id="cb27-3" data-line-number="3">       <span class="kw">aes</span>(<span class="dt">x =</span> perc_disadvan, <span class="dt">y =</span> average_sat_math, <span class="dt">color =</span> size)) <span class="op">+</span></a>
<a class="sourceLine" id="cb27-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.25</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb27-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_parallel_slopes</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb27-6" data-line-number="6"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Percent economically disadvantaged&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Math SAT Score&quot;</span>, </a>
<a class="sourceLine" id="cb27-7" data-line-number="7">       <span class="dt">color =</span> <span class="st">&quot;School size&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Parallel slopes model&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:numxcatx-comparison-2"></span>
<img src="ModernDive_files/figure-html/numxcatx-comparison-2-1.png" alt="Comparison of interaction and parallel slopes models for Massachusetts schools." width="\textwidth" />
<p class="caption">
FIGURE 1.8: Comparison of interaction and parallel slopes models for Massachusetts schools.
</p>
</div>
<p>Look closely at the left-hand plot of Figure <a href="1-3-mult-reg-related-topics.html#fig:numxcatx-comparison-2">1.8</a> corresponding to an interaction model. While the slopes are indeed different, they do not differ <em>by much</em> and are nearly identical. Now compare the left-hand plot with the right-hand plot corresponding to a parallel slopes model. The two models don’t appear all that different. So in this case, it can be argued that the additional complexity of the interaction model is <em>not warranted</em>. Thus following Occam’s Razor, we should prefer the “simpler” parallel slopes model. Let’s explicitly define what “simpler” means in this case. Let’s compare the regression tables for the interaction and parallel slopes models in Tables <a href="1-3-mult-reg-related-topics.html#tab:model2-interaction">1.12</a> and <a href="1-3-mult-reg-related-topics.html#tab:model2-parallel-slopes">1.13</a>.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">model_<span class="dv">2</span>_interaction &lt;-<span class="st"> </span><span class="kw">lm</span>(average_sat_math <span class="op">~</span><span class="st"> </span>perc_disadvan <span class="op">*</span><span class="st"> </span>size, </a>
<a class="sourceLine" id="cb28-2" data-line-number="2">                          <span class="dt">data =</span> MA_schools)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3"><span class="kw">get_regression_table</span>(model_<span class="dv">2</span>_interaction)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:model2-interaction">TABLE 1.12: </span>Interaction model regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
594.327
</td>
<td style="text-align:right;">
13.288
</td>
<td style="text-align:right;">
44.726
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
568.186
</td>
<td style="text-align:right;">
620.469
</td>
</tr>
<tr>
<td style="text-align:left;">
perc_disadvan
</td>
<td style="text-align:right;">
-2.932
</td>
<td style="text-align:right;">
0.294
</td>
<td style="text-align:right;">
-9.961
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
-3.511
</td>
<td style="text-align:right;">
-2.353
</td>
</tr>
<tr>
<td style="text-align:left;">
sizemedium
</td>
<td style="text-align:right;">
-17.764
</td>
<td style="text-align:right;">
15.827
</td>
<td style="text-align:right;">
-1.122
</td>
<td style="text-align:right;">
0.263
</td>
<td style="text-align:right;">
-48.899
</td>
<td style="text-align:right;">
13.371
</td>
</tr>
<tr>
<td style="text-align:left;">
sizelarge
</td>
<td style="text-align:right;">
-13.293
</td>
<td style="text-align:right;">
13.813
</td>
<td style="text-align:right;">
-0.962
</td>
<td style="text-align:right;">
0.337
</td>
<td style="text-align:right;">
-40.466
</td>
<td style="text-align:right;">
13.880
</td>
</tr>
<tr>
<td style="text-align:left;">
perc_disadvan:sizemedium
</td>
<td style="text-align:right;">
0.146
</td>
<td style="text-align:right;">
0.371
</td>
<td style="text-align:right;">
0.393
</td>
<td style="text-align:right;">
0.694
</td>
<td style="text-align:right;">
-0.585
</td>
<td style="text-align:right;">
0.877
</td>
</tr>
<tr>
<td style="text-align:left;">
perc_disadvan:sizelarge
</td>
<td style="text-align:right;">
0.189
</td>
<td style="text-align:right;">
0.323
</td>
<td style="text-align:right;">
0.586
</td>
<td style="text-align:right;">
0.559
</td>
<td style="text-align:right;">
-0.446
</td>
<td style="text-align:right;">
0.824
</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1">model_<span class="dv">2</span>_parallel_slopes &lt;-<span class="st"> </span><span class="kw">lm</span>(average_sat_math <span class="op">~</span><span class="st"> </span>perc_disadvan <span class="op">+</span><span class="st"> </span>size, </a>
<a class="sourceLine" id="cb29-2" data-line-number="2">                              <span class="dt">data =</span> MA_schools)</a>
<a class="sourceLine" id="cb29-3" data-line-number="3"><span class="kw">get_regression_table</span>(model_<span class="dv">2</span>_parallel_slopes)</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:model2-parallel-slopes">TABLE 1.13: </span>Parallel slopes regression table
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
588.19
</td>
<td style="text-align:right;">
7.607
</td>
<td style="text-align:right;">
77.325
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
573.23
</td>
<td style="text-align:right;">
603.15
</td>
</tr>
<tr>
<td style="text-align:left;">
perc_disadvan
</td>
<td style="text-align:right;">
-2.78
</td>
<td style="text-align:right;">
0.106
</td>
<td style="text-align:right;">
-26.120
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
-2.99
</td>
<td style="text-align:right;">
-2.57
</td>
</tr>
<tr>
<td style="text-align:left;">
sizemedium
</td>
<td style="text-align:right;">
-11.91
</td>
<td style="text-align:right;">
7.535
</td>
<td style="text-align:right;">
-1.581
</td>
<td style="text-align:right;">
0.115
</td>
<td style="text-align:right;">
-26.74
</td>
<td style="text-align:right;">
2.91
</td>
</tr>
<tr>
<td style="text-align:left;">
sizelarge
</td>
<td style="text-align:right;">
-6.36
</td>
<td style="text-align:right;">
6.923
</td>
<td style="text-align:right;">
-0.919
</td>
<td style="text-align:right;">
0.359
</td>
<td style="text-align:right;">
-19.98
</td>
<td style="text-align:right;">
7.26
</td>
</tr>
</tbody>
</table>
<p>Observe how the regression table for the interaction model has 2 more rows (6 versus 4). This reflects the additional “complexity” of the interaction model over the parallel slopes model.</p>
<p>Furthermore, note in Table <a href="1-3-mult-reg-related-topics.html#tab:model2-interaction">1.12</a> how the <em>offsets for the slopes</em> <code>perc_disadvan:sizemedium</code> being 0.146 and <code>perc_disadvan:sizelarge</code> being 0.189 are small relative to the <em>slope for the baseline group</em> of small schools of <span class="math inline">\(-2.932\)</span>. In other words, all three slopes are similarly negative: <span class="math inline">\(-2.932\)</span> for small schools, <span class="math inline">\(-2.786\)</span> <span class="math inline">\((=-2.932 + 0.146)\)</span> for medium schools, and <span class="math inline">\(-2.743\)</span> <span class="math inline">\((=-2.932 + 0.189)\)</span> for large schools. These results are suggesting that irrespective of school size, the relationship between average math SAT scores and the percent of the student body that is economically disadvantaged is similar and, alas, quite negative.</p>
<p>What you have just performed is a rudimentary <em>model selection</em>: choosing which model fits data best among a set of candidate models. The model selection we performed used the “eyeball test”: qualitatively looking at visualizations to choose a model. In the next subsection, you’ll once again perform the same model selection, but this time using a numerical approach via the <span class="math inline">\(R^2\)</span> (pronounced “R-squared”) value.</p>
<!--
TODO: Make this into a learning check
Given that intercepts in the parallel slopes model in the right-hand plot of Figure \@ref(fig:numxcatx-comparison-2) are also similar as well, it can be argued that an even better model is one without the categorical variable school size. 
-->
</div>
<div id="rsquared" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Model selection using R-squared</h3>
<p>At the end of the previous section in Figure <a href="1-3-mult-reg-related-topics.html#fig:numxcatx-comparison-2">1.8</a> you compared an interaction model with a parallel slopes model, where both models attempted to explain <span class="math inline">\(y\)</span> = the average math SAT score for various high schools in Massachusetts. In Tables <a href="1-3-mult-reg-related-topics.html#tab:model2-interaction">1.12</a> and <a href="1-3-mult-reg-related-topics.html#tab:model2-parallel-slopes">1.13</a>, we observed that the interaction model was “more complex” in that the regression table had 6 rows versus the 4 rows of the parallel slopes model.</p>
<p>Most importantly however, when comparing the left and right-hand plots of Figure <a href="1-3-mult-reg-related-topics.html#fig:numxcatx-comparison-2">1.8</a>, we observed that the three lines corresponding to small, medium, and large high schools were not that different. Given this similarity, we stated it could be argued that the “simpler” parallel slopes model should be favored.</p>
<p>In this section, we’ll mimic the model selection we just performed using the qualitative “eyeball test”, but this time using a numerical and quantitative approach. Specifically, we’ll use the <span class="math inline">\(R^2\)</span> summary statistic (pronounced “R-squared”), also called the “coefficient of determination”. But first, we must introduce one new concept: the <em>variance</em> of a numerical variable.</p>
<p>We’ve previously studied two summary statistics of the <em>spread</em> (or <em>variation</em>) of a numerical variable: the standard deviation when studying the normal distribution in <a href="#appendix-normal-curve"><strong>??</strong></a> and the interquartile range (IQR) when studying boxplots in Section <a href="#geomboxplot"><strong>??</strong></a>. We now introduce a third summary statistic of spread: the <em>variance</em>. The variance is merely the standard deviation squared and it can be computed in R using the <code>var()</code> summary function within <code>summarize()</code>. If you would like to see the formula, see <a href="#appendix-sd-variance"><strong>??</strong></a>.</p>
<p>Recall that to get: 1) the observed values <span class="math inline">\(y\)</span>, 2) the fitted values <span class="math inline">\(\widehat{y}\)</span> from a regression model, and 3) the resulting residuals <span class="math inline">\(y - \widehat{y}\)</span>, we can apply the <code>get_regression_points()</code> function our saved model, in this case <code>model_2_interaction</code>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="kw">get_regression_points</span>(model_<span class="dv">2</span>_interaction) </a></code></pre></div>
<pre><code># A tibble: 332 x 6
      ID average_sat_math perc_disadvan size   average_sat_math_hat residual
   &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;
 1     1              516          21.5 medium                 517.    -0.67
 2     2              514          22.7 large                  519.    -4.77
 3     3              534          14.6 large                  541.    -6.99
 4     4              581           6.3 large                  564.    17.2 
 5     5              592          10.3 large                  553.    39.2 
 6     6              576          10.3 large                  553.    23.2 
 7     7              504          25.6 large                  511.    -6.82
 8     8              505          15.2 large                  539.   -34.3 
 9     9              481          23.8 small                  525.   -43.5 
10    10              513          25.5 large                  511.     1.91
# … with 322 more rows</code></pre>
<p>Let’s now use the <code>var()</code> summary function within a <code>summarize()</code> to compute the variance of these three terms:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1"><span class="kw">get_regression_points</span>(model_<span class="dv">2</span>_interaction) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb32-2" data-line-number="2"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">var_y =</span> <span class="kw">var</span>(average_sat_math), </a>
<a class="sourceLine" id="cb32-3" data-line-number="3">                      <span class="dt">var_y_hat =</span> <span class="kw">var</span>(average_sat_math_hat), </a>
<a class="sourceLine" id="cb32-4" data-line-number="4">                      <span class="dt">var_residual =</span> <span class="kw">var</span>(residual))</a></code></pre></div>
<pre><code># A tibble: 1 x 3
  var_y var_y_hat var_residual
  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
1 3691.     2580.        1111.</code></pre>
<p>Observe that the variance of <span class="math inline">\(y\)</span> is equal to the variance of <span class="math inline">\(\widehat{y}\)</span> plus the variance of the residuals. But what do these three terms tell us individually?</p>
<p>First, the variance of <span class="math inline">\(y\)</span> (denoted as <span class="math inline">\(var(y)\)</span>) tells us how much do Massachusetts high schools differ in average math SAT scores. The goal of regression modeling is to fit a model that hopefully <em>explains</em> this variation. In other words, we want to understand what factors explain why certain schools have high math SAT scores, while others have low scores. This is independent of the model; this is just data. In other words, whether we fit an interaction or parallel slopes model, <span class="math inline">\(var(y)\)</span> remains the same.</p>
<p>Second, the variance of <span class="math inline">\(\widehat{y}\)</span> (denoted as <span class="math inline">\(var(\widehat{y})\)</span>) tells us how much the fitted values from our interaction model vary. That is to say, after accounting for (1) the percentage of the study body that is socioeconomically disadvantaged and (2) school size in an interaction model, how much do our model’s explanations of average math SAT scores vary?</p>
<p>Third, the variance of the residuals tells us how much do “the left-overs” from the model vary. Observe how the points in the left-hand plot of Figure <a href="1-3-mult-reg-related-topics.html#fig:numxcatx-comparison-2">1.8</a> scatter around the three lines. Say instead all the points fell <em>exactly</em> on one of the three lines. Then all residuals would be zero and hence the variance of the residuals would be zero.</p>
<p>We’re now ready to introduce <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[
R^2 = \frac{var(\widehat{y})}{var(y)}
\]</span></p>
<p>It is <em>the proportion of the spread/variation of the outcome variable <span class="math inline">\(y\)</span> that is explained by our model</em>, where our model’s explanatory power is embedded in the fitted values <span class="math inline">\(\widehat{y}\)</span>. Furthermore, since it can be mathematically proven that <span class="math inline">\(0 \leq var(\widehat{y}) \leq var(y)\)</span> (a fact we leave for an advanced class on regression), we are guaranteed that:</p>
<p><span class="math display">\[
0 \leq R^2 \leq 1
\]</span></p>
<p><span class="math inline">\(R^2\)</span> can be interpreted as follows:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R^2\)</span> values of 0 tell us that our model explains 0% of the variation in <span class="math inline">\(y\)</span>. Say we fit a model to the Massachusetts high school data and obtained <span class="math inline">\(R^2 = 0\)</span>. This would be telling us that the combination of explanatory variables <span class="math inline">\(x\)</span> we used and model form we chose (interaction or parallel slopes) tell us <em>nothing</em> about average math SAT scores. The model is a poor fit.</li>
<li><span class="math inline">\(R^2\)</span> values of 1 tell us that our model explains 100% of the variation in <span class="math inline">\(y\)</span>. Say we fit a model to the Massachusetts high school data and obtained <span class="math inline">\(R^2 = 1\)</span>. This would be telling us that the combination of explanatory variables <span class="math inline">\(x\)</span> we used and model form we chose (interaction or parallel slopes) tell us <em>everything we need to know</em> about average math SAT scores.</li>
</ol>
<p>In practice however, <span class="math inline">\(R^2\)</span> values of 1 almost never occur. Think about it in the context of Massachusetts high schools. There are an infinite number of factors that influence why certain high schools perform well on SAT’s on average while others don’t perform well. The idea that a human-designed statistical model can capture all the heterogeneity of all high school students in Massachusetts is bordering on hubris. However, even if such models are not perfect, they may still prove useful in determining educational policy. A general principle of modeling we should keep in mind is a famous quote by eminent statistician George Box: <a href="https://jamesclear.com/all-models-are-wrong">“All models are wrong, but some are useful.”</a></p>
<p>Let’s repeat the above calculations for the parallel slopes model and compare them in Table <a href="1-3-mult-reg-related-topics.html#tab:model2-r-squared">1.14</a>.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:model2-r-squared">TABLE 1.14: </span>Comparing variances from interaction and parallel slopes models for MA school data
</caption>
<thead>
<tr>
<th style="text-align:left;">
model
</th>
<th style="text-align:right;">
var_y
</th>
<th style="text-align:right;">
var_y_hat
</th>
<th style="text-align:right;">
var_residual
</th>
<th style="text-align:right;">
r_squared
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Interaction
</td>
<td style="text-align:right;">
3691
</td>
<td style="text-align:right;">
2580
</td>
<td style="text-align:right;">
1111
</td>
<td style="text-align:right;">
0.699
</td>
</tr>
<tr>
<td style="text-align:left;">
Parallel slopes
</td>
<td style="text-align:right;">
3691
</td>
<td style="text-align:right;">
2579
</td>
<td style="text-align:right;">
1112
</td>
<td style="text-align:right;">
0.699
</td>
</tr>
</tbody>
</table>
<p>Observe how the <span class="math inline">\(R^2\)</span> values are near identical at around 0.699 = 69.9%. In other words, the <em>additional complexity</em> of the interaction model only improves our <span class="math inline">\(R^2\)</span> value by a near zero amount. Thus, we are inclined to favor the “simpler” parallel slopes model.</p>
<p>Now let’s repeat this <span class="math inline">\(R^2\)</span> comparison between interaction and parallel slopes model for our models of <span class="math inline">\(y\)</span> = teaching score for UT Austin professors which you visually compared in Figure <a href="1-3-mult-reg-related-topics.html#fig:recall-parallel-vs-interaction">1.7</a>. We compare these values in Table <a href="1-3-mult-reg-related-topics.html#tab:model1-r-squared">1.15</a></p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:model1-r-squared">TABLE 1.15: </span>Comparing variances from interaction and parallel slopes models for UT Austin data
</caption>
<thead>
<tr>
<th style="text-align:left;">
model
</th>
<th style="text-align:right;">
var_y
</th>
<th style="text-align:right;">
var_y_hat
</th>
<th style="text-align:right;">
var_residual
</th>
<th style="text-align:right;">
r_squared
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Interaction
</td>
<td style="text-align:right;">
0.296
</td>
<td style="text-align:right;">
0.015
</td>
<td style="text-align:right;">
0.281
</td>
<td style="text-align:right;">
0.051
</td>
</tr>
<tr>
<td style="text-align:left;">
Parallel slopes
</td>
<td style="text-align:right;">
0.296
</td>
<td style="text-align:right;">
0.012
</td>
<td style="text-align:right;">
0.284
</td>
<td style="text-align:right;">
0.039
</td>
</tr>
</tbody>
</table>
<p>Observe how the <span class="math inline">\(R^2\)</span> values are now very different! In other words, since the <em>additional complexity</em> of the interaction model over the parallel slopes model improves our <span class="math inline">\(R^2\)</span> value by a relatively large amount (0.051 versus 0.039, which is an increase of about 31.5%), it could be argued that the additional complexity is warranted.</p>
<p>As a final note, we can also use the third of our <code>get_regression()</code> wrapper functions, <code>get_regression_summaries()</code>, to quickly automate calculating <span class="math inline">\(R^2\)</span> for both the interaction and parallels slopes models for <span class="math inline">\(y\)</span> = average math SAT score for Massachusetts high schools.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="co"># R-squared for interaction model:</span></a>
<a class="sourceLine" id="cb34-2" data-line-number="2"><span class="kw">get_regression_summaries</span>(model_<span class="dv">2</span>_interaction)</a></code></pre></div>
<pre><code># A tibble: 1 x 9
  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs
      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1     0.699         0.694 1107.  33.3  33.6      151.       0     5   332</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1"><span class="co"># R-squared for parallel slopes model:</span></a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="kw">get_regression_summaries</span>(model_<span class="dv">2</span>_parallel_slopes)</a></code></pre></div>
<pre><code># A tibble: 1 x 9
  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs
      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1     0.699         0.696 1109.  33.3  33.5      254.       0     3   332</code></pre>
</div>
<div id="correlationcoefficient2" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Correlation coefficient</h3>
<p>Recall from Table <a href="1-2-model3.html#tab:model3-correlation">1.9</a> that the correlation coefficient between <code>income</code> in thousands of dollars and credit card <code>debt</code> was 0.464. What if instead we looked at the correlation coefficient between <code>income</code> and credit card <code>debt</code>, but where <code>income</code> was in dollars and not thousands of dollars? This can be done by multiplying <code>income</code> by 1000.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1">credit_ch6 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(debt, income) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">income =</span> income <span class="op">*</span><span class="st"> </span><span class="dv">1000</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-3" data-line-number="3"><span class="st">  </span><span class="kw">cor</span>()</a></code></pre></div>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:cor-credit-2">TABLE 1.16: </span>Correlation between income (in dollars) and credit card debt
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
debt
</th>
<th style="text-align:right;">
income
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
debt
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0.464
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:right;">
0.464
</td>
<td style="text-align:right;">
1.000
</td>
</tr>
</tbody>
</table>
<p>We see it is the same! We say that the correlation coefficient is <em>invariant to linear transformations</em>. The correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> will be the same as the correlation between <span class="math inline">\(a\cdot x + b\)</span> and <span class="math inline">\(y\)</span> for any numerical values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
</div>
<div id="simpsonsparadox" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Simpson’s Paradox</h3>
<p>Recall in Section <a href="1-2-model3.html#model3">1.2</a>, we saw the two seemingly contradictory results when studying the relationship between credit card <code>debt</code> and <code>income</code>. On the one hand, the right hand plot of Figure <a href="1-2-model3.html#fig:2numxplot1">1.5</a> suggested that the relationship between credit card <code>debt</code> and <code>income</code> was <em>positive</em>. We re-display this in Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot1-repeat">1.9</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:2numxplot1-repeat"></span>
<img src="ModernDive_files/figure-html/2numxplot1-repeat-1.png" alt="Relationship between credit card debt and income." width="\textwidth" />
<p class="caption">
FIGURE 1.9: Relationship between credit card debt and income.
</p>
</div>
<p>On the other hand, the multiple regression results in Table <a href="1-2-model3.html#tab:model3-table-output">1.10</a> suggested that the relationship between <code>debt</code> and <code>income</code> was <em>negative</em>. We re-display this information in Table <a href="1-3-mult-reg-related-topics.html#tab:model3-table-output-repeat">1.17</a>.</p>
<table class="table" style="font-size: 16px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:model3-table-output-repeat">TABLE 1.17: </span>Multiple regression results
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std_error
</th>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p_value
</th>
<th style="text-align:right;">
lower_ci
</th>
<th style="text-align:right;">
upper_ci
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
-385.179
</td>
<td style="text-align:right;">
19.465
</td>
<td style="text-align:right;">
-19.8
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-423.446
</td>
<td style="text-align:right;">
-346.912
</td>
</tr>
<tr>
<td style="text-align:left;">
credit_limit
</td>
<td style="text-align:right;">
0.264
</td>
<td style="text-align:right;">
0.006
</td>
<td style="text-align:right;">
45.0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.253
</td>
<td style="text-align:right;">
0.276
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:right;">
-7.663
</td>
<td style="text-align:right;">
0.385
</td>
<td style="text-align:right;">
-19.9
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-8.420
</td>
<td style="text-align:right;">
-6.906
</td>
</tr>
</tbody>
</table>
<p>Observe how the slope for <code>income</code> is <span class="math inline">\(-7.663\)</span> and, most importantly for now, it is negative. This contradicts our observation in Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot1-repeat">1.9</a> that the relationship is positive. How can this be? Recall the interpretation of the slope for <code>income</code> in the context of a multiple regression model: <em>taking into account all the other explanatory variables in our model</em>, for every increase of one unit in <code>income</code> (i.e., $1000), there is an associated decrease of on average $7.663 in <code>debt</code>.</p>
<p>In other words, while in <em>isolation</em>, the relationship between <code>debt</code> and <code>income</code> may be positive, when taking into account <code>credit_limit</code> as well, this relationship becomes negative. These seemingly paradoxical results are due to a phenomenon aptly named <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox"><em>Simpson’s Paradox</em></a>. Simpson’s Paradox occurs when trends that exist for the data in aggregate either disappear or reverse when the data are broken down into groups.</p>
<p>Let’s show how Simpson’s Paradox manifests itself in the <code>credit_ch6</code> data. Let’s first visualize the distribution of the numerical explanatory variable <code>credit_limit</code> with a histogram in Figure <a href="1-3-mult-reg-related-topics.html#fig:credit-limit-quartiles">1.10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:credit-limit-quartiles"></span>
<img src="ModernDive_files/figure-html/credit-limit-quartiles-1.png" alt="Histogram of credit limits and brackets." width="\textwidth" />
<p class="caption">
FIGURE 1.10: Histogram of credit limits and brackets.
</p>
</div>
<p>The vertical dashed lines are the <em>quartiles</em> that cut up the variable <code>credit_limit</code> into four equally sized groups. Let’s think of these quartiles as converting our numerical variable <code>credit_limit</code> into a categorical variable “<code>credit_limit</code> bracket” with four levels. This means that</p>
<ol style="list-style-type: decimal">
<li>25% of credit limits were between $0 and $3088. Let’s assign these 100 people to the “low” <code>credit_limit</code> bracket.</li>
<li>25% of credit limits were between $3088 and $4622. Let’s assign these 100 people to the “medium-low” <code>credit_limit</code> bracket.</li>
<li>25% of credit limits were between $4622 and $5873. Let’s assign these 100 people to the “medium-high” <code>credit_limit</code> bracket.</li>
<li>25% of credit limits were over $5873. Let’s assign these 100 people to the “high” <code>credit_limit</code> bracket.</li>
</ol>
<p>Now in Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot4">1.11</a> let’s re-display two versions of the scatterplot of <code>debt</code> and <code>income</code> from Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot1-repeat">1.9</a>, but with a slight twist:</p>
<ol style="list-style-type: decimal">
<li>The left-hand plot shows the regular scatterplot and the single regression line, just as you saw in Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot1-repeat">1.9</a>.</li>
<li>The right-hand plot shows the <em>colored scatterplot</em>, where the color aesthetic is mapped to “<code>credit_limit</code> bracket.” Furthermore, there are now four separate regression lines.</li>
</ol>
<p>In other words, the location of the 400 points are the same in both scatterplots, but the right-hand plot shows an additional variable of information: <code>credit_limit</code> bracket.</p>
<div class="figure" style="text-align: center"><span id="fig:2numxplot4"></span>
<img src="ModernDive_files/figure-html/2numxplot4-1.png" alt="Relationship between credit card debt and income by credit limit bracket." width="\textwidth" />
<p class="caption">
FIGURE 1.11: Relationship between credit card debt and income by credit limit bracket.
</p>
</div>
<p>The left-hand plot of Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot4">1.11</a> focuses on the relationship between <code>debt</code> and <code>income</code> in <em>aggregate</em>. It is suggesting that overall there exists a positive relationship between <code>debt</code> and <code>income</code>. However, the right-hand plot of Figure <a href="1-3-mult-reg-related-topics.html#fig:2numxplot4">1.11</a> focuses on the relationship between <code>debt</code> and <code>income</code> <em>broken down by <code>credit_limit</code> bracket</em>. In other words, we focus on four <em>separate</em> relationships between <code>debt</code> and <code>income</code>: one for the “low” <code>credit_limit</code> bracket, one for the “medium-low” <code>credit_limit</code> bracket, and so on.</p>
<p>Observe in the right-hand plot that the relationship between <code>debt</code> and <code>income</code> is clearly negative for the “medium-low” and “medium-high” <code>credit_limit</code> brackets, while the relationship is somewhat flat for the “low” <code>credit_limit</code> bracket. The only <code>credit_limit</code> bracket where the relationship remains positive is for the “high” <code>credit_limit</code> bracket. However, this relationship is less positive than in the relationship in aggregate, since the slope is shallower than the slope of the regression line in the left-hand plot.</p>
<p>In this example of Simpson’s Paradox, the <code>credit_limit</code> is a <em>confounding variable</em> of the relationship between credit card <code>debt</code> and <code>income</code> as we defined in Subsection <a href="#correlation-is-not-causation"><strong>??</strong></a>. Thus, <code>credit_limit</code> needs to be accounted for in any appropriate model for the relationship between <code>debt</code> and <code>income</code>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-2-model3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="1-4-mult-reg-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/moderndive/moderndive_book/edit/master/06-multiple-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
